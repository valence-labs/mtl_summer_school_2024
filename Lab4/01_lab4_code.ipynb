{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT9J1WKT_e4C"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QKjBzLO8RMUp"
      },
      "outputs": [],
      "source": [
        "!pip install cmapPy\n",
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yLud4fiuKwK4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gzip\n",
        "import re\n",
        "from cmapPy.pandasGEXpress.parse import parse\n",
        "from matplotlib import pyplot as plt\n",
        "import umap\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "79fwIsIKu-2M"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwRRTSHIKyQ1"
      },
      "source": [
        "## The data\n",
        "\n",
        "### Download\n",
        "Download expression and necessary metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dD4UakOhJ7Ze"
      },
      "outputs": [],
      "source": [
        "!wget https://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70138/suppl/GSE70138_Broad_LINCS_inst_info_2017-03-06.txt.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eYAd6YkBOmnS"
      },
      "outputs": [],
      "source": [
        "!wget https://ftp.ncbi.nlm.nih.gov/geo/series/GSE70nnn/GSE70138/suppl/GSE70138_Broad_LINCS_Level2_GEX_n345976x978_2017-03-06.gctx.gz\n",
        "!gunzip GSE70138_Broad_LINCS_Level2_GEX_n345976x978_2017-03-06.gctx.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuKZjQDmK-xp"
      },
      "source": [
        "### Import the metadata\n",
        "\n",
        "Please note the columns of particular interest\n",
        "- inst_id: ID col, used to join the metadata with the data\n",
        "- cell_id: cell line\n",
        "- det_plate: can be used as the experimental batch, i.e. all the wells with the same det_plate were ran together\n",
        "\n",
        "And especially columns relating to the perturbation:\n",
        "- pert_iname: human readable name of perturbant\n",
        "- pert_dose: concentration of perturbant\n",
        "- pert_time: time between perturbation and measurement CHECK!\n",
        "- pert_type: whether this perturbant is a negative control or treatment, and whether it's a small compound or genetic perturbation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iouwDa8iKWB2"
      },
      "outputs": [],
      "source": [
        "inst_info = pd.read_csv(\"GSE70138_Broad_LINCS_inst_info_2017-03-06.txt.gz\", sep='\\t')\n",
        "inst_info.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIseR7NhO4GN"
      },
      "source": [
        "In case you were wondering, the -666 values are how the lincs l1000 data chose to indicate NA values\n",
        "\n",
        "### Import the expression data\n",
        "\n",
        "Here we will work with lincs L1000 data, level 2.\n",
        "\n",
        "> Note that level 2 is _not_ the standardly recommend starting point as it often rather used with normalization, processing (such as imputation), batch correction (level 4), and aggregation (level 5). However we utilize this because\n",
        "- it surfaces many of the realistic challenges of working with _any_ expression data\n",
        "- it is much smaller, which is useful for a quick lab\n",
        "\n",
        "\n",
        "This data contains a matrix with\n",
        "\"landmark genes\" in the rows\n",
        "and \"samples\" or \"wells\" in the columns.\n",
        "The values are the the measured abundance of\n",
        "the mRNAs derived from the given\n",
        "landmark gene in a given sample.\n",
        "\n",
        "Landmark genes are a subset of ~1000 genes\n",
        "that are sufficient to predict much of the\n",
        "variation in all 20k genes. Only these 1000\n",
        "are measured in the l1000 data for cost reasons. This contrasts with _sequencing_ based transcriptomics, which is not selective for a subset of target genes.\n",
        "\n",
        "Find more information [in the paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5990023/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_UbYF8_bO2QL"
      },
      "outputs": [],
      "source": [
        "gene_abundance = parse(\"GSE70138_Broad_LINCS_Level2_GEX_n345976x978_2017-03-06.gctx\",\n",
        "                       convert_neg_666=True).data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2h6YTRocKPg3"
      },
      "outputs": [],
      "source": [
        "gene_abundance.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiDd_xmaUBG0"
      },
      "source": [
        "#### Data distribution\n",
        "Transcriptomics data is generally not normally distributed, while many of the losses or metrics we use in ML work better with normally distributed data. The l1000 is not count data, but we can still see it's not normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SFlOmr74KahX"
      },
      "outputs": [],
      "source": [
        "# note the long tail\n",
        "_ = plt.hist(gene_abundance.iloc[:,:300].values.flatten(), bins=300)\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xlabel(\"Gene Abundance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcc9Bc3pp8DZ"
      },
      "source": [
        "#### Normalization\n",
        "> Note: the following normalization and scaling differs from the recomendation for l1000 for the sake of simplicity and showing general steps which apply to many transcriptomics types. For l1000 outside of this lab, you can simply take the level 4 or 5 pre-normalized data, or take a look at the author's [write up of \"Level 3 - Normalization (NORM)\"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5990023/#S40title). For sequencing data, a few good options are to either use raw counts and adjust loss & metrics for a negative binomial distribution _or_ one could perform variance stabilizing transformation with e.g. [DESeq2](https://bioconductor.org/packages/release/bioc/html/DESeq2.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XYUNc4wSr2bq"
      },
      "outputs": [],
      "source": [
        "# log transform\n",
        "gene_normalized = np.log(gene_abundance.values) + 0.001\n",
        "# scale each sample to same total abundance\n",
        "gene_normalized = gene_normalized / np.mean(gene_normalized, axis=0) * np.mean(gene_normalized)\n",
        "# center at 0\n",
        "gene_normalized = gene_normalized - np.mean(gene_normalized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E7WFgW1EsiUK"
      },
      "outputs": [],
      "source": [
        "_ = plt.hist(gene_normalized[:,:30000].flatten(), bins=300)\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xlabel(\"Log-Centred Gene Abundance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LjltGdcVyF_"
      },
      "source": [
        "While the transformations have not resulted in a perfect normal distribution, it is bell shaped and does not have extreme outliers. We'll consider this good enough to work with.\n",
        "\n",
        "Now let's look at subgroups and structures in the data with a UMAP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QyuX-wPtwuKc"
      },
      "outputs": [],
      "source": [
        "# visualization tends to be clearer (and faster)\n",
        "# when we don't take the _full_ perturbation data\n",
        "N = 10000\n",
        "\n",
        "# setup and run UMAP\n",
        "reducer = umap.UMAP()\n",
        "embedding = reducer.fit_transform(gene_normalized[:,:N].T)\n",
        "embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qg4oNSOIxNvi"
      },
      "outputs": [],
      "source": [
        "# rearrange and join with metadata for seaborn\n",
        "dat = pd.DataFrame(embedding)\n",
        "dat[\"inst_id\"] = gene_abundance.columns[:N]\n",
        "dat = dat.merge(inst_info, on='inst_id')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QcdcB5szxmw3"
      },
      "outputs": [],
      "source": [
        "dat.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n6xdllK0plK"
      },
      "source": [
        "#### Visualize normalized, but unaligned\n",
        "\n",
        "First we see that cell line, unsurprisingly, has a dominant effect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VQ8PCtz6xvnD"
      },
      "outputs": [],
      "source": [
        "ax = sns.scatterplot(dat, x=0, y=1, hue=\"cell_id\", s=3)\n",
        "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
        "plt.xlabel(\"UMAP 0\")\n",
        "plt.ylabel(\"UMAP 1\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoEd2tqksBpV"
      },
      "source": [
        "And within each cell line, the batch (or plate) has a large effect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ksdzfvBD0_qF"
      },
      "outputs": [],
      "source": [
        "ax = sns.scatterplot(dat, x=0, y=1, hue=\"det_plate\", s=3, palette=\"Paired\")\n",
        "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
        "plt.xlabel(\"UMAP 0\")\n",
        "plt.ylabel(\"UMAP 1\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYFc6HEttC6J"
      },
      "source": [
        "Good to know, we'll come back to the batch effect after embedding the data\n",
        "\n",
        "### Embedding\n",
        "#### Why?\n",
        "Transcriptomics data can be _very_ noisy (especially single cell data, as a large fraction of transcripts are near the sensitivity/detection limit). Moreover, most transcriptomcis assays include 20k genes, often with high gene-gene correlations. Thus, embedding the transcriptomics data into a lower dimensional space can reduce noise, simplify comparisons and increase the utility of the data. We acknowledge that the selection of 978 landmark genes already works _somewhat like an embedding_ and exactly this lab would work without an embedding step; however, for broader applicability to other types of transcriptomics (and especially other, either noisier or unstructured bio-assay types) we wanted to include this step.\n",
        "\n",
        "#### How?\n",
        "While the field of transcriptomics foundation models is under\n",
        "rapid development, e.g. ([scGPT](https://www.nature.com/articles/s41592-024-02201-0), [scFoundation](https://www.biorxiv.org/content/10.1101/2023.05.29.542705v3), [Geneformer](https://www.nature.com/articles/s41586-023-06139-9), [Universal Cell Embeddings](https://www.biorxiv.org/content/10.1101/2023.11.28.568918v1)), the foundation models are generally sequencing specific and few have been independently benchmarked to date. From the benchmarks that\n",
        "have been done, it is clear that [scVI](https://www.nature.com/articles/s41592-018-0229-2), a variational autoencoder, remains a\n",
        "strong baseline. None of the above actually are designed for L1000 data, and for today we need something that is fast to train,\n",
        "so we'll take a vanilla **variational autoencoder** for simplicity.\n",
        "\n",
        "#### Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4WgJWOf3kcky"
      },
      "outputs": [],
      "source": [
        "# simple tabular data loader\n",
        "class L1000Dataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.X = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X.T)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        sample = (self.X[:,idx], )\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "anI6A_CnkwmA"
      },
      "outputs": [],
      "source": [
        "joint_dataset = L1000Dataset(gene_normalized)\n",
        "total_len = len(joint_dataset)\n",
        "len_train = int(total_len * .9)\n",
        "len_val = total_len - len_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn9wNg7lZrxO"
      },
      "source": [
        "> Note. In this lab we are not concerned with _generalization_.\n",
        "We're simply using the embedding to reduce noise, and measuring\n",
        "similarity (within set). Most ML projects with transcriptomics\n",
        "data _will_ be concerned with generalization and should consider\n",
        "a split that reserves whole experimental batches\n",
        "(such as 'gem wells' or 'plates')\n",
        "of data for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E767pYdYk6ny"
      },
      "outputs": [],
      "source": [
        "training_dataset, validation_dataset = random_split(joint_dataset, lengths=[len_train, len_val])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qFm05PFon_32"
      },
      "outputs": [],
      "source": [
        "batch_size = 512\n",
        "training_dataloader = DataLoader(training_dataset, batch_size=batch_size,\n",
        "                                 shuffle=True, num_workers=0)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size,\n",
        "                                   shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HGneUEK75u6"
      },
      "source": [
        "#### The Variational Autoencoder Model\n",
        "This is an autoencoder, which has an encoder that compresses the information\n",
        "from the input down to a smaller latent space, and then a decoder that expands back to the input dimensions. It is trained\n",
        "to reconstruct the input as precisely as possible.\n",
        "For the _variational_ part, a sampling\n",
        "step is applied, the encoder predicts a mean and standard deviation,\n",
        "from which a sample is taken and fed to the decoder during training.\n",
        "This allows VAEs to work as a _generative_ model, for which they are better\n",
        "known. However VAEs are also useful for embedding as the sampling step\n",
        "both has a regularizing function and importantly causes a smoother\n",
        "latent space.\n",
        "\n",
        "> Our implemntation borrowed from: https://github.com/lyeoni/pytorch-mnist-VAE/blob/master/pytorch-mnist-VAE.ipynb and https://github.com/AntixK/PyTorch-VAE/blob/master/models/vanilla_vae.py; both of which are good resources on VAEs generally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r0ziAIfjoUsR"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.fc_1 = nn.Linear(x_dim, h_dim)\n",
        "        self.fc_2a = nn.Linear(h_dim, z_dim)\n",
        "        self.fc_2b = nn.Linear(h_dim, z_dim)\n",
        "        self.fc_3 = nn.Linear(z_dim, h_dim)\n",
        "        self.fc_4 = nn.Linear(h_dim, x_dim)\n",
        "\n",
        "    def encoder(self, x):\n",
        "        h = F.relu(self.fc_1(x))\n",
        "        return self.fc_2a(h), self.fc_2b(h) # mu, log_var\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps * std + mu # z sample\n",
        "\n",
        "    def decoder(self, z):\n",
        "        h = F.relu(self.fc_3(z))\n",
        "        return self.fc_4(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encoder(x.view(-1, 978))\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        return self.decoder(z), mu, log_var\n",
        "\n",
        "\n",
        "vae = VAE(x_dim=978, h_dim=512, z_dim=256)\n",
        "vae.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "t9CHAXnmvjRa"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(vae.parameters(), lr=1e-4)\n",
        "# return reconstruction error + KL divergence losses\n",
        "def loss_function(recon_x, x, mu, log_var):\n",
        "    mse = F.mse_loss(recon_x, x.view(-1, 978))\n",
        "    kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
        "\n",
        "    return mse + kld_loss * 0.0002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A5i9mv3uvojU"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    vae.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, ) in enumerate(training_dataloader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        recon_batch, mu, log_var = vae(data)\n",
        "        loss = loss_function(recon_batch, data, mu, log_var)\n",
        "\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(training_dataloader.dataset),\n",
        "                100. * batch_idx / len(training_dataloader), loss.item() / len(data)))\n",
        "    print(f'====> Epoch: {epoch} Average loss: {train_loss / len(training_dataloader.dataset):.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bfRoffG-vpYa"
      },
      "outputs": [],
      "source": [
        "def val():\n",
        "    vae.eval()\n",
        "    val_loss= 0\n",
        "    with torch.no_grad():\n",
        "        for (data,) in validation_dataloader:\n",
        "            data = data.to(device)\n",
        "            recon, mu, log_var = vae(data)\n",
        "\n",
        "            # sum up batch loss\n",
        "            val_loss += loss_function(recon, data, mu, log_var).item()\n",
        "\n",
        "    val_loss /= len(validation_dataloader.dataset)\n",
        "    print(f'====> Val set loss: {val_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T6zVfukOvt_U"
      },
      "outputs": [],
      "source": [
        "# this will take several minutes, a great time\n",
        "# to read or catch up on the theory\n",
        "for epoch in range(1, 3):\n",
        "    train(epoch)\n",
        "    val()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qTrd64av7Yv"
      },
      "source": [
        "Check if output distribution is reasonable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H96dpVSLv_c6"
      },
      "outputs": [],
      "source": [
        "for (data, ) in validation_dataloader:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "y7hB7JYkwAzH"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    res = vae(data.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-VDlLXAywA15"
      },
      "outputs": [],
      "source": [
        "# do we get a bell curve / match to ori dist?\n",
        "_ = plt.hist(res[0].cpu().numpy().flatten(), bins=50)\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xlabel(\"Predicted Log-Centred Gene Abundance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ndBYj3YNwA5A"
      },
      "outputs": [],
      "source": [
        "# compared to exact input?\n",
        "plt.scatter(data.cpu().numpy().flatten(), res[0].cpu().numpy().flatten())\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.xlabel(\"Ground Truth\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XyVmqlueXp9"
      },
      "source": [
        "We have a notable correlation, especially for more extreme values.\n",
        "\n",
        "In a performance oriented project we would perform hyperparameter tuning\n",
        "using downstream metrics to select the best model. But for now,\n",
        "we'll continue with this one and embed all the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C4tCezD8edM5"
      },
      "outputs": [],
      "source": [
        "out = []\n",
        "# all the data, unshuffled\n",
        "joint_dataloader = DataLoader(joint_dataset, batch_size=batch_size,\n",
        "                                 shuffle=False, num_workers=0)\n",
        "for (data, ) in joint_dataloader:\n",
        "    with torch.no_grad():\n",
        "        out.append(vae(data.to(device))[1].cpu().numpy())  # 1 is the embedding, AKA mu\n",
        "vae_embedding = np.concatenate(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACjxuLcOtxOX"
      },
      "source": [
        "### Experimental batch correction\n",
        "Even high throughput biological assays have much smaller capacity than the number of samples we wish to measure in drug discovery. Thus, the data is gathered in 'batches', for instance, everything that fits on a 384 well plate at the same time. These batches necessarily share some technical (e.g. sequencing depth) and biological (e.g. exact cell age, response to time of day) co-variation that is often greater than the effect of some individual perturbants.\n",
        "\n",
        "How best to make downstream modelling tasks robust to the batches\n",
        "in which experimental data was\n",
        "collected or even integrate data from disparate studies is an evolving field\n",
        "(e.g. [Harmony](https://www.nature.com/articles/s41592-019-0619-0),\n",
        "[TVN](https://www.biorxiv.org/content/10.1101/161422v1.abstract),\n",
        "[sysVI](https://www.biorxiv.org/content/10.1101/2023.11.03.565463v2),\n",
        "[InfoCORE](https://arxiv.org/abs/2312.00718) to name only a few).\n",
        "Here, we simply center each batch on the control mean and scale it by the\n",
        "control standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cTbx2baxo2b0"
      },
      "outputs": [],
      "source": [
        "# first, helper functions\n",
        "def find_controls(inst_info_batch):\n",
        "    \"\"\"parses the metadata to identify which controls can be used for centering\"\"\"\n",
        "    # the plates (batches) have _either_\n",
        "    # compounds with DMSO (\"ctl_vehicle\") as a negative, centering control\n",
        "    comp_control = 'ctl_vehicle'\n",
        "    comp_treatment = 'trt_cp'\n",
        "    comp_pert_types = (comp_control, comp_treatment)\n",
        "    # OR genetic perturbations with a non-targeting guide (\"ctl_vector\") as a negative, centering control\n",
        "    genetic_control = 'ctl_vector'\n",
        "    genetic_treatment = 'trt_xpr'\n",
        "    genetic_null = 'ctl_untrt'\n",
        "    genetic_pert_types = (genetic_null, genetic_control, genetic_treatment)\n",
        "    # what did we actually get?\n",
        "    batch_pert_types = tuple(np.sort(inst_info_batch.pert_type.unique()))\n",
        "    if batch_pert_types == comp_pert_types:\n",
        "        return inst_info_batch.pert_type == comp_control\n",
        "    elif batch_pert_types == genetic_pert_types:\n",
        "        return inst_info_batch.pert_type == genetic_control\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"unknown perturbation types {tuple(inst_info_batch.pert_type.unique())}\"\n",
        "        )\n",
        "\n",
        "\n",
        "def center_scale_on_controls(dat_batch_tup, inst_info_batch_tup):\n",
        "    \"\"\"centers each batch on the control mean and scales by the control\n",
        "    standard deviation.\n",
        "\n",
        "    Thus the (average) control will have a mean of 0 and sd of 1 afterwards,\n",
        "    while perturbed samples may vary.\"\"\"\n",
        "\n",
        "    # drop batch that came from groupby\n",
        "    _, dat_batch = dat_batch_tup\n",
        "    _, inst_info_batch = inst_info_batch_tup\n",
        "    # select controls\n",
        "    controls = dat_batch.loc[find_controls(inst_info_batch)]\n",
        "    # calculate mean & sd for each 'feature'\n",
        "    control_mean = np.mean(controls, axis=0)\n",
        "    control_sd = np.std(controls, axis=0)\n",
        "    # normalize all data (subtract mean, divide by sd)\n",
        "    return (dat_batch - control_mean) / control_sd\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c9dtlz5tt_9i"
      },
      "outputs": [],
      "source": [
        "# put meta data (inst_info) and normalized gene abundance\n",
        "# into pandas dfs with matching order & index\n",
        "ordered_inst_info = inst_info.set_index(\"inst_id\")\n",
        "ordered_inst_info = ordered_inst_info.loc[gene_abundance.columns, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u4ss6ibYuQsq"
      },
      "outputs": [],
      "source": [
        "splitable = pd.DataFrame(gene_normalized).T\n",
        "splitable.index = ordered_inst_info.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AlpLRyZAuN5N"
      },
      "outputs": [],
      "source": [
        "# loop through batches and center\n",
        "out = []\n",
        "for dat_batch, inst_info_batch in zip(splitable.groupby(ordered_inst_info['det_plate']),\n",
        "                                     ordered_inst_info.groupby('det_plate')):\n",
        "    out.append(center_scale_on_controls(dat_batch, inst_info_batch))\n",
        "# re-concatenate batches and restore original order\n",
        "bc_vae_embedding = pd.concat(out).loc[ordered_inst_info.index]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da4Gwx5rAlN_"
      },
      "source": [
        "Let's see how the batch normalization changed the groupings in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r2R9p98GAxpD"
      },
      "outputs": [],
      "source": [
        "# UMAP, as above\n",
        "embedding = reducer.fit_transform(bc_vae_embedding[:N])\n",
        "dat = pd.DataFrame(embedding)\n",
        "dat[\"inst_id\"] = gene_abundance.columns[:N]\n",
        "dat = dat.merge(inst_info, on='inst_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RxDYv8PKBDG-"
      },
      "outputs": [],
      "source": [
        "# cell types colored\n",
        "ax = sns.scatterplot(dat, x=0, y=1, hue=\"cell_id\", s=3)\n",
        "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c-682wFXBR3p"
      },
      "outputs": [],
      "source": [
        "ax = sns.scatterplot(dat, x=0, y=1, hue=\"det_plate\", s=3)\n",
        "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEiklYMiBkZl"
      },
      "source": [
        "The mixing is imperfect, but nevertheless the data points are far better mixed than before embedding and batch correction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr-ntjH3wdzx"
      },
      "source": [
        "### Apply embeddings to target deconvolution\n",
        "We will try and use these embeddings to identify\n",
        "(or in this case verify) potential targets\n",
        "(or disrupted pathways) for everolimus, an inhibitor of\n",
        "MTOR.\n",
        "\n",
        "We can work with the hypothesis, that a drug inhibiting\n",
        "a protein target will induce a similar phenotype to a\n",
        "genetic knock out of said protein target. Whether this\n",
        "phenotype is measured by phenomics, transcriptomics,\n",
        "or any other high through put assay, if we can make\n",
        "centered embeddings, we can simply find similar phenotypes\n",
        "with cosine similarity and consider these candidate targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3UkFzSD_Doo4"
      },
      "outputs": [],
      "source": [
        "def mean_cosine_similarity(mat):\n",
        "    \"\"\"calculates the mean of non-self pairwise cosine similarities in mat\"\"\"\n",
        "    res = cosine_similarity(mat)\n",
        "    upper_triangular_mask = np.triu(np.ones(res.shape, dtype=bool), 1)\n",
        "    return np.mean(res[upper_triangular_mask])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jp9NeeQ0wgxa"
      },
      "outputs": [],
      "source": [
        "# confirm our compound of interest is present in data\n",
        "compound = \"everolimus\"\n",
        "ordered_inst_info.loc[ordered_inst_info.pert_iname == compound].pert_id.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DNhZ08AEkJO"
      },
      "source": [
        "Before we can dive in to actually comparing things, we have some decisions to make. In particular, what cell type and what dose should we take. We'll\n",
        "also have to clean up some data inconsistencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FsG9H-eNFBub"
      },
      "outputs": [],
      "source": [
        "# combine non-exact matches in cell line naming, and save set for selecting cell lines\n",
        "# with available knockouts\n",
        "ordered_inst_info[\"cell_line\"] = np.array([re.sub('\\..*', '', x) for x in ordered_inst_info.cell_id])\n",
        "cell_lines_2_keep = set(ordered_inst_info[ordered_inst_info.pert_type == \"trt_xpr\"].cell_line.unique())\n",
        "cell_lines_2_keep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_U_eJvjTFgEK"
      },
      "outputs": [],
      "source": [
        "# select all instances of compound perturbation on potential comparable cell lines\n",
        "mask = (ordered_inst_info.pert_iname == compound) & ordered_inst_info.cell_id.isin(cell_lines_2_keep)\n",
        "comp_dat = bc_vae_embedding.loc[mask]\n",
        "comp_meta = ordered_inst_info.loc[mask]\n",
        "# equalize rounding so dosages match\n",
        "comp_meta.loc[:, 'pert_dose_rounded'] = np.round(comp_meta.pert_dose, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qBINGi0aHPcX"
      },
      "outputs": [],
      "source": [
        "# calcute the cosine similarity between all replicates within a given\n",
        "# cell line and dosage.\n",
        "# the idea here, is if replicates don't correlate, then there's likely\n",
        "# no consistent effect for compound <-> gene comparisons\n",
        "# in biological terms, these might be cases where the compound is doesed\n",
        "# too low to have an effect or the protein target is not expressed\n",
        "flat = comp_dat.groupby([comp_meta.cell_id, comp_meta.pert_dose_rounded]).apply(\n",
        "    mean_cosine_similarity)\n",
        "flat = pd.DataFrame(flat).reset_index()\n",
        "# reshape to cell line X dose\n",
        "pivoted = pd.pivot(flat, columns='pert_dose_rounded', index='cell_id')\n",
        "pivoted.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZmnxkT3nITcV"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(pivoted, vmin=-1, vmax=1, cmap=\"seismic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwojKO6AwisV"
      },
      "source": [
        "This compound has a fairly consistent effect across cell\n",
        "lines, nevertheless we will focus on a couple of the strongest.\n",
        "\n",
        "#### Find the most similar genetic knock outs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kyCJS4QgImnj"
      },
      "outputs": [],
      "source": [
        "def query_gene_knockouts(compound, target_cell_line, min_dose, max_dose,\n",
        "                         embedding, metadata):\n",
        "    \"\"\"finds genes in L1000 dataset that are most similar to query comound\n",
        "    under specified cell line and dosage constraints\"\"\"\n",
        "\n",
        "    # prep binary masks to later select...\n",
        "    # ... cell line\n",
        "    cell_mask = metadata.cell_line == target_cell_line\n",
        "    # ... all genes\n",
        "    genes_mask = metadata.pert_type == \"trt_xpr\"\n",
        "    # ... compound of interest\n",
        "    comp_mask = metadata.pert_iname == compound\n",
        "    # ... doses of interest\n",
        "    dose_mask = (min_dose <= metadata.pert_dose) & (metadata.pert_dose <=  max_dose)\n",
        "\n",
        "    # get data subsets\n",
        "    comp_dat = embedding.loc[cell_mask & comp_mask & dose_mask]\n",
        "\n",
        "    sub_info = metadata.loc[cell_mask & genes_mask]\n",
        "    sub_dat = embedding.loc[cell_mask & genes_mask]\n",
        "\n",
        "    # aggregate all genes\n",
        "    g_references = sub_dat.groupby(sub_info.pert_iname).mean()\n",
        "    # get query compound\n",
        "    comp_query = comp_dat.mean()\n",
        "\n",
        "    # calculate cosine similarities\n",
        "    ready = pd.concat([g_references, pd.DataFrame(comp_query).T])\n",
        "    return pd.Series(cosine_similarity(ready)[-1], index=ready.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z8vmF8ACmMu"
      },
      "source": [
        "Now we'll query for the gene knockouts that show the most similar perturbation\n",
        "effect to everolimus, in some of the cell lines and dosage that showed the\n",
        "most reproducibility for everolimus above. Adjust cell line and dosages\n",
        "as you see fit. Other compounds with less global effects are likely more\n",
        "sensitive to such choices.\n",
        "\n",
        "### Result time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C_HlrhqQI2ZJ"
      },
      "outputs": [],
      "source": [
        "# top hits in cell line A549\n",
        "query_gene_knockouts(compound, \"A549\", min_dose=0, max_dose=2,\n",
        "                     embedding=bc_vae_embedding, metadata=ordered_inst_info).sort_values(ascending=False)[1:11]  # 0th index is always the compound itself, skip it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ncg-jxhsI6FJ"
      },
      "outputs": [],
      "source": [
        "# top hits in A375\n",
        "query_gene_knockouts(compound, \"A375\", min_dose=.11, max_dose=2,\n",
        "                     embedding=bc_vae_embedding, metadata=ordered_inst_info).sort_values(ascending=False)[1:11]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7ub0xtRWJv67"
      },
      "outputs": [],
      "source": [
        "# top hits in YAPC\n",
        "query_gene_knockouts(compound, \"YAPC\", min_dose=0, max_dose=0.15,\n",
        "                     embedding=bc_vae_embedding, metadata=ordered_inst_info).sort_values(ascending=False)[1:11]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNaHyGqWJ9WU"
      },
      "source": [
        "#### Thoughts\n",
        "1. It's good to see MTOR robustly at or near the top of the list.\n",
        "\n",
        "2. Other consistent hits should be checked if they could be indirect effects\n",
        "(e.g. in the pathway with MTOR); or whether they might be off target effects.\n",
        "It is likely helpful to consult a database such as [signor](https://signor.uniroma2.it/), [reactome](https://reactome.org/) or [corum](https://mips.helmholtz-muenchen.de/corum/) to find expected indirect interactions.\n",
        "\n",
        "**Congratulations! You made it!**\n",
        "\n",
        "-----------------\n",
        "\n",
        "\n",
        "## Additional exercises\n",
        "If you have extra time, we'd encourage you to do one of the following\n",
        "\n",
        "\n",
        "1) Optimization.\n",
        "This lab was designed to be a minimalistic, but nevertheless an end to end, target deconvolution analysis. At each step, we consistently chose a simple\n",
        "and \"good enough\" option, but did not try to optimize any of them.\n",
        "\n",
        "The exercize: pick your favorite step of\n",
        "this lab (e.g. normalization, batch correction, embedding, similarity measurement) and try and optimize it.\n",
        "\n",
        "2) Extend functionality.\n",
        "This lab was minimalistic, there are surely many things that could\n",
        "be added to strengthen the analysis.\n",
        "\n",
        "The exercize: make a prioritized and justified list of what you would\n",
        "add next.\n",
        "\n",
        "3) Follow up experiments on potential off target effects.\n",
        "We ended with a list of candidate drug targets and didn't make final\n",
        "decisions on what was a chance correlation, an indirect effect, or an\n",
        "actual target.\n",
        "\n",
        "The exercise: previous labs have given you tools to asses drug-protein\n",
        "interactions. Use these, or at least consider what you would best use,\n",
        "to check some of the candidate interactions further.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}